import streamlit as st
import os
import json
import google.generativeai as genai
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
import uuid
from datetime import datetime
import streamlit.components.v1 as components
import asyncio
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import traceback
import glob
import re

def generate_search_query(prompt, conversation_history):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ä¼šè©±å±¥æ­´ã‹ã‚‰ã€FAISSæ¤œç´¢ã«æœ€é©ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹"""
    
    # ä¼šè©±å±¥æ­´ã‚’æ•´å½¢
    history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history])

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    prompt_template = f"""
ã‚ãªãŸã¯å„ªç§€ãªæ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ä¼šè©±å±¥æ­´ã¨æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†æã—ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’å¼•ãå‡ºã™ãŸã‚ã®ã€ç°¡æ½”ã‹ã¤åŠ¹æœçš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€åˆ¶ç´„æ¡ä»¶ã€‘
- æœ€ã‚‚é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
- æ¤œç´¢ã‚¯ã‚¨ãƒªä»¥å¤–ã®ä½™è¨ˆãªæ–‡ç« ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’æ­£ç¢ºã«åæ˜ ã—ã¦ãã ã•ã„ã€‚

ã€ä¼šè©±å±¥æ­´ã€‘
{history_str}

ã€æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘
{prompt}

ã€ç”Ÿæˆã™ã¹ãæ¤œç´¢ã‚¯ã‚¨ãƒªã€‘
"""
    
    try:
        # Geminiã«æ¤œç´¢ã‚¯ã‚¨ãƒªã®ç”Ÿæˆã‚’ä¾é ¼
        response = model.generate_content(prompt_template)
        search_query = response.text.strip()
        # st.info(f"ç”Ÿæˆã•ã‚ŒãŸæ¤œç´¢ã‚¯ã‚¨ãƒª: `{search_query}`") # ãƒ‡ãƒãƒƒã‚°ç”¨
        return search_query
    except Exception as e:
        st.warning(f"æ¤œç´¢ã‚¯ã‚¨ãƒªã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}ã€‚å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return prompt


# --- å®šæ•° ---
SIMILARITY_THRESHOLD = 0.7 # é¡ä¼¼åº¦è©•ä¾¡ã®ã—ãã„å€¤ã‚’å†åº¦æœ‰åŠ¹åŒ–
FEEDBACK_LOG_FILE = "feedback.log"

def log_feedback(message_id, user_prompt, assistant_response, feedback):
    """
    ä¼šè©±ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã—ã¾ã™ã€‚
    """
    timestamp = datetime.now().isoformat()
    log_entry = {
        "timestamp": timestamp,
        "message_id": message_id,
        "user_prompt": user_prompt,
        "assistant_response": assistant_response,
        "feedback": feedback
    }
    with open(FEEDBACK_LOG_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\\n")

# --- åˆæœŸè¨­å®š ---
st.title("ã„ã¤ã§ã‚‚ã—ã‚…ã‚“ã•ã‚“")

# Streamlitã®secretsã‹ã‚‰APIã‚­ãƒ¼ã‚’è¨­å®š
try:
    # éåŒæœŸå‡¦ç†ã®åˆæœŸåŒ–å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€å…¨ã¦ã®å‡¦ç†ã®å‰ã«ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’è¨­å®š
    try:
        asyncio.get_running_loop()
    except RuntimeError:
        asyncio.set_event_loop(asyncio.new_event_loop())

    api_key = st.secrets.get("GEMINI_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
    if not api_key:
        raise KeyError("API key not found")
    genai.configure(api_key=api_key)
    
    # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¨ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
    model = genai.GenerativeModel("gemini-2.5-pro")
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        google_api_key=api_key
    )

except KeyError:
    st.error("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlit Cloudã®è¨­å®šã§GEMINI_API_KEYã¾ãŸã¯GOOGLE_API_KEYã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ ---
KNOWLEDGE_SOURCES = [
    ("ã‚ªãƒ¼ãƒ€ãƒ¼ãƒãƒ¼ãƒˆç¾å®Ÿå‰µé€ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ", "**/*.txt"),
]
FAISS_INDEX_PATH = "data/faiss_index_v3" # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å¼·åˆ¶å†æ§‹ç¯‰ã™ã‚‹ãŸã‚ãƒ‘ã‚¹ã‚’å¤‰æ›´

def build_and_save_faiss_index(embeddings):
    st.info("çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’å†æ§‹ç¯‰ã—ã¦ã„ã¾ã™...")
    try:
        source_directory, pattern = KNOWLEDGE_SOURCES[0]
        
        # globã‚’ç›´æ¥ä½¿ã„ã€å†å¸°çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ç¢ºå®Ÿãªæ–¹æ³•ã«å¤‰æ›´
        search_path = os.path.join(source_directory, pattern)
        all_file_paths = glob.glob(search_path, recursive=True)

        if not all_file_paths:
            st.error(f"'{source_directory}' å†…ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚globãƒ‘ã‚¿ãƒ¼ãƒ³: '{search_path}'")
            st.stop()
        
        # ç™ºè¦‹ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ãƒ‡ãƒãƒƒã‚°ç”¨ã«è¡¨ç¤º
        with st.expander(f"ç™ºè¦‹ã•ã‚ŒãŸ {len(all_file_paths)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆï¼ˆæœ€åˆã®30ä»¶ï¼‰"):
            st.code('\n'.join(sorted(all_file_paths)[:30]))

        # å€‹åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’TextLoaderã§èª­ã¿è¾¼ã‚€
        documents = []
        for file_path in all_file_paths:
            try:
                loader = TextLoader(file_path, encoding='utf-8')
                documents.extend(loader.load())
            except Exception as e:
                st.warning(f"ãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        
        st.info(f"'{source_directory}' ã‹ã‚‰ {len(documents)} ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸã€‚")
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
        chunks = text_splitter.split_documents(documents)
        
        st.info("FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰ã¨ä¿å­˜ã‚’é–‹å§‹ã—ã¾ã™...")
        db = FAISS.from_documents(chunks, embeddings)
        os.makedirs(os.path.dirname(FAISS_INDEX_PATH), exist_ok=True)
        db.save_local(FAISS_INDEX_PATH)
        st.success(f"æ–°ã—ã„çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ '{FAISS_INDEX_PATH}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        return db
    except Exception as e:
        st.error(f"FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ§‹ç¯‰ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\\n{traceback.format_exc()}")
        st.stop()


# --- FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰ ---
@st.cache_resource
def load_faiss_index(path, _embeddings):
    """FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆå­˜åœ¨ã—ãªã„ã‹å£Šã‚Œã¦ã„ã‚Œã°å†æ§‹ç¯‰ï¼‰"""
    try:
        # ã¾ãšã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ç¢ºå®Ÿã«è¨­å®š
        loop = asyncio.get_event_loop()
        if loop.is_closed():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    if os.path.exists(path):
        try:
            return FAISS.load_local(path, _embeddings, allow_dangerous_deserialization=True)
        except Exception as e:
            st.warning(f"æ—¢å­˜ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ ({e})ã€‚å†æ§‹ç¯‰ã‚’è©¦ã¿ã¾ã™ã€‚")
            return build_and_save_faiss_index(_embeddings)
    else:
        return build_and_save_faiss_index(_embeddings)

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒŠãƒ¬ãƒƒã‚¸ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¡¨ç¤º ---
st.sidebar.title("âš™ï¸ ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æƒ…å ±")
try:
    source_directory, pattern = KNOWLEDGE_SOURCES[0]
    search_path = os.path.join(source_directory, pattern)
    all_file_paths = sorted(glob.glob(search_path, recursive=True))

    st.sidebar.success(f"**èª­ã¿è¾¼ã¿å…ƒãƒ•ã‚©ãƒ«ãƒ€:**\\n`{source_directory}`")

    with st.sidebar.expander(f"**èª­ã¿è¾¼ã¿å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(all_file_paths)}ä»¶**"):
        st.markdown("---")
        # ãƒ‘ã‚¹ã‚’æ•´å½¢ã—ã¦è¡¨ç¤º
        display_text = "\\n".join([f"- `{os.path.relpath(p)}`" for p in all_file_paths])
        st.markdown(display_text)

except Exception as e:
    st.sidebar.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")


db = load_faiss_index(FAISS_INDEX_PATH, embeddings)

# --- AIã«ã‚ˆã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆ ---
def generate_search_query(prompt, conversation_history):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ä¼šè©±å±¥æ­´ã‹ã‚‰ã€FAISSæ¤œç´¢ã«æœ€é©ãªã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹"""
    
    # ä¼šè©±å±¥æ­´ã‚’æ•´å½¢
    history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history])

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    prompt_template = f"""
ã‚ãªãŸã¯å„ªç§€ãªæ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ä¼šè©±å±¥æ­´ã¨æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†æã—ã€ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’å¼•ãå‡ºã™ãŸã‚ã®ã€ç°¡æ½”ã‹ã¤åŠ¹æœçš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€åˆ¶ç´„æ¡ä»¶ã€‘
- æœ€ã‚‚é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
- æ¤œç´¢ã‚¯ã‚¨ãƒªä»¥å¤–ã®ä½™è¨ˆãªæ–‡ç« ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’æ­£ç¢ºã«åæ˜ ã—ã¦ãã ã•ã„ã€‚

ã€ä¼šè©±å±¥æ­´ã€‘
{history_str}

ã€æœ€å¾Œã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‘
{prompt}

ã€ç”Ÿæˆã™ã¹ãæ¤œç´¢ã‚¯ã‚¨ãƒªã€‘
"""
    
    try:
        # Geminiã«æ¤œç´¢ã‚¯ã‚¨ãƒªã®ç”Ÿæˆã‚’ä¾é ¼
        response = model.generate_content(prompt_template)
        search_query = response.text.strip()
        # st.info(f"ç”Ÿæˆã•ã‚ŒãŸæ¤œç´¢ã‚¯ã‚¨ãƒª: `{search_query}`") # ãƒ‡ãƒãƒƒã‚°ç”¨
        return search_query
    except Exception as e:
        st.warning(f"æ¤œç´¢ã‚¯ã‚¨ãƒªã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}ã€‚å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return prompt


# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = [{
        "role": "assistant",
        "content": """åƒ•ã¯ã—ã‚…ã‚“ã•ã‚“ã®ã‚¯ãƒ­ãƒ¼ãƒ³ã§ã™ã€‚ã—ã‚…ã‚“ã•ã‚“ãŒæ•™ãˆã¦ãã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã‚ãªãŸã®è³ªå•ã«ç­”ãˆã¡ã‚ƒã†ã‚ˆï¼å¼•ãå¯„ã›ã®æ³•å‰‡ãƒ»ã‚ªãƒ¼ãƒ€ãƒ¼ãƒãƒ¼ãƒˆã‚’å­¦ã¶ä¸­ã§ç–‘å•ã‚„äººç”Ÿç›¸è«‡ãªã©ã‚ã‚Œã°ãªã‚“ãªã‚Šãƒãƒ£ãƒƒãƒˆã‹ã‚‰æ•™ãˆã¦ãã ã•ã„ï¼

â€»ã‚ãªãŸãŒè³ªå•ã—ãŸã“ã¨ã¯ã„ã‹ãªã‚‹ã“ã¨ã§ã‚ã£ã¦ã‚‚ã€ã—ã‚…ã‚“ã•ã‚“ã‚„ä»–ã®äººã«ã¯è¦‹ãˆãªã„ã‹ã‚‰ã€å®‰å¿ƒã—ã¦ã­ï¼""",
        "id": str(uuid.uuid4()),
    }]
if "scroll_to_bottom" not in st.session_state:
    st.session_state.scroll_to_bottom = False
if 'feedback_given' not in st.session_state:
    st.session_state.feedback_given = {}

# --- ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º ---
for i, msg in enumerate(st.session_state.messages):
    avatar_url = "assets/avatar.png" if msg["role"] == "assistant" else "user"
    with st.chat_message(msg["role"], avatar=avatar_url):
        st.markdown(msg["content"])
        
        # --- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿèƒ½ ---
        # AIã®å¿œç­”ã§ã€ã‹ã¤æœ€åˆã®æŒ¨æ‹¶ã§ã¯ãªã„å ´åˆã«ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
        if msg["role"] == "assistant" and i > 0:
            feedback_status = st.session_state.feedback_given.get(msg["id"])
            
            if not feedback_status:
                cols = st.columns([1, 1, 8])
                with cols[0]:
                    if st.button("ğŸ‘", key=f"good_{msg['id']}", help="ã“ã®å›ç­”ã«æº€è¶³"):
                        user_prompt = ""
                        if i > 0 and st.session_state.messages[i-1]["role"] == "user":
                            user_prompt = st.session_state.messages[i-1]["content"]
                        log_feedback(msg['id'], user_prompt, msg['content'], "good")
                        st.session_state.feedback_given[msg['id']] = 'good'
                        st.toast("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼")
                        st.rerun()
                
                with cols[1]:
                    if st.button("ğŸ‘", key=f"bad_{msg['id']}", help="ã“ã®å›ç­”ã«ä¸æº€"):
                        user_prompt = ""
                        if i > 0 and st.session_state.messages[i-1]["role"] == "user":
                            user_prompt = st.session_state.messages[i-1]["content"]
                        log_feedback(msg['id'], user_prompt, msg['content'], "bad")
                        st.session_state.feedback_given[msg['id']] = 'bad'
                        st.toast("æ”¹å–„ã®ãŸã‚ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€æ„Ÿè¬ã—ã¾ã™ã€‚")
                        st.rerun()
            else:
                st.markdown(
                    f"<span style='color: #4A4A4A;'>{'ğŸ‘' if feedback_status == 'good' else 'ğŸ‘'} ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ¸ˆã¿</span>", 
                    unsafe_allow_html=True
                )

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã€ã‹ã¤å‚ç…§å…ƒæƒ…å ±ãŒã‚ã‚‹å ´åˆ
        if msg["role"] == "assistant" and "sources" in msg and msg["sources"]:
            with st.expander("å‚ç…§å…ƒãƒ•ã‚¡ã‚¤ãƒ«"):
                for item in msg["sources"]:
                    doc = item["doc"]
                    score = item["score"]
                    reason = item["reason"]
                    
                    st.markdown(f"**ãƒ•ã‚¡ã‚¤ãƒ«å:** `{doc.metadata.get('source', 'N/A')}`")
                    st.markdown(f"**é¸æŠç†ç”±:** {reason}")
                    
                    # ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ã£ã¦å‚ç…§ç®‡æ‰€ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ã«
                    content_html = f"""
                        <div style="background-color: #262730; border-radius: 5px; padding: 10px; height: 150px; overflow-y: auto; border: 1px solid #333;">
                            <pre style="white-space: pre-wrap; word-wrap: break-word; color: #FAFAFA; font-family: 'Source Code Pro', monospace;">{doc.page_content}</pre>
                        </div>
                    """
                    components.html(content_html, height=170)
                    st.divider()

# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ› ---
if prompt := st.chat_input("è³ªå•ã‚„ç›¸è«‡ã—ãŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­"):
    st.session_state.messages.append({"role": "user", "content": prompt, "id": str(uuid.uuid4())})
    
    with st.chat_message("user", avatar="user"):
        st.markdown(prompt)

    with st.chat_message("assistant", avatar="assets/avatar.png"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # 1. æœ€é©ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
            with st.spinner("æœ€é©ãªæ¤œç´¢æ–¹æ³•ã‚’è€ƒãˆã¦ã„ã¾ã™..."):
                 search_query = generate_search_query(prompt, st.session_state.messages)

            # 2. ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒªã§FAISSã‚’æ¤œç´¢
            with st.spinner("ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™..."):
                docs_with_scores = db.similarity_search_with_score(search_query, k=10)
            
            # 3. æ¤œç´¢çµæœã‚’æ•´å½¢
            source_texts = []
            source_docs_with_reasons = []
            is_relevant_info_found = False

            if docs_with_scores:
                for doc, score in docs_with_scores:
                    # ã“ã“ã§ã™ã§ã«é–¢é€£æ€§ãŒã‚ã‚‹ã‹ã©ã†ã‹ã®å¤§ã¾ã‹ãªåˆ¤æ–­ã‚’å…¥ã‚Œã‚‹
                    # ä¾‹: ã‚¹ã‚³ã‚¢ãŒä¸€å®šä»¥ä¸Šã€ã¾ãŸã¯ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ãªã©
                    # ä»Šå›ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ä¸Šä½5ä»¶ã¯é–¢é€£ã‚ã‚Šã¨ã¿ãªã™
                    if len(source_docs_with_reasons) < 5:
                         source_docs_with_reasons.append({
                            "doc": doc,
                            "reason": f"AIã®åˆ¤æ–­ã«ã‚ˆã‚‹é–¢é€£å€™è£œ (ã‚¹ã‚³ã‚¢: {score:.4f})"
                        })
                         source_texts.append(doc.page_content)
                         is_relevant_info_found = True

                # é–¢é€£ã™ã‚‹ã‚‚ã®ãŒ1ã¤ã§ã‚‚ã‚ã‚Œã°ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
                if is_relevant_info_found:
                    context = "--- é–¢é€£æƒ…å ± ---\n"
                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹æƒ…å ±ã‚’ä¸Šä½5ä»¶ã«åˆ¶é™
                    for item in source_docs_with_reasons:
                        context += item["doc"].page_content + "\n\n"
                else:
                    context = "--- é–¢é€£æƒ…å ± ---\nãªã—"

            # --- ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™ ---
            with open("å¼•ãå¯„ã›ã‚¢ãƒ—ãƒª/docs/system_prompt.md", "r", encoding="utf-8") as f:
                final_system_prompt = f.read()

            final_system_prompt += f"\n\n{context}"
            
            # --- APIå‘¼ã³å‡ºã—ã¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° ---
            history = []
            for m in st.session_state.messages[:-1]:
                role = 'user' if m['role'] == 'user' else 'model'
                history.append({'role': role, 'parts': [{'text': m['content']}]})

            chat = model.start_chat(history=history)
            
            prompt_with_context = f"{final_system_prompt}\n\nuser: {prompt}\nassistant:"

            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã—ã¦å¿œç­”ã‚’ç”Ÿæˆ
            stream = chat.send_message(
                prompt,
                stream=True,
                generation_config=genai.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=4096,
                )
            )

            for chunk in stream:
                if chunk.text:
                    full_response += chunk.text
                    message_placeholder.markdown(full_response + "â–Œ")
            
            # æœ€çµ‚çš„ãªå¿œç­”ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦è¡¨ç¤º
            message_placeholder.markdown(full_response)

        except Exception as e:
            st.error(f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            full_response = "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            message_placeholder.markdown(full_response)

        # å¿œç­”ã¨å‚ç…§å…ƒã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã«è¿½åŠ 
        st.session_state.messages.append({
        "role": "assistant",
        "content": full_response,
        "sources": source_docs_with_reasons,
        "id": str(uuid.uuid4()),
    })

    # --- è‡ªå‹•ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã¨å†å®Ÿè¡Œ ---
    st.session_state.scroll_to_bottom = True
    st.rerun()

# --- è‡ªå‹•ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã®å®Ÿè¡Œ ---
if st.session_state.get("scroll_to_bottom"):
    components.html(
        "<script>window.scrollTo(0, document.body.scrollHeight);</script>",
        height=0
    )
    st.session_state.scroll_to_bottom = False 